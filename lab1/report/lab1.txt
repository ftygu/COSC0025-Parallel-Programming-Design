实验目标
实验目标概括如下：
Cache 优化与矩阵向量内积计算：本实验旨在通过计算给定 n×n
n×n 矩阵的每一列与给定向量的内积，探究和实践两种算法设计思路——一种是逐列访问元素的基础算法，另一种是考虑cache优化的算法。通过编程实现这两种思路，并利用高精度计时测试它们的执行时间，旨在展示cache优化对算法性能的显著提升。
超标量优化与数列求和：本实验探索计算n个数求和的两种算法：一种是逐个累加的基础算法，另一种是适合超标量架构的指令级并行算法，例如两路链式累加或递归相加算法。通过编程实现这些算法并比较它们的性能，实验旨在揭示指令级并行优化对性能的影响。
性能分析工具的应用：利用性能分析工具（如prof和uprof）对上述算法进行深入分析，通过运行计时和事件计数的方法，量化普通算法与优化算法之间的性能差异。这一步骤旨在提供一个量化基础，以科学地评价不同算法设计思路的效率。
实验设计及分析

1
针对给定的问题，由于矩阵在内存中存储时按照行有限的顺序存储的，也就是说，在内存中的矩
阵是按行紧密排列的。因此，对于原始朴素的逐列访问算法来说，CPU 会一次读入连续的一段数据到
缓存中，其中可能只包含需要计算一个元素，因此当计算该列的第二个元素的时候，CPU 又需要到更
低的缓存或内存中去读取所需要的元素，而访存的时间相较于运算来说，开销是很大的，这会在很大
程度上降低程序运行的效率。
因此我们考虑改进算法，采用逐行访问的 cache 优化算法，即充分利用每次读入的数据，将当前
读入的缓存中的一行数据全部进行计算，然后累加到结果数组的对应位置，虽然在这个过程中并没有
能够直接计算出结果，但是极大利用了 cache 中的缓存数据，减少去内存中寻找数据的访存时间。
同时，为了能够降低循环访问过程中，条件判断，指令跳转等额外开销，我们对于逐行访问的算
法进行了进一步优化，采用循环展开的方法，在一次循环中，同时计算 2 个位置的值，可以利用多条
流水线同时作业，发挥 CPU 超标量计算的性能。

为此，我们分别设计了三种算法，并在 ARM 架构的华为云鲲鹏服务器上进行测试，实验测试数
据如表1所示
结合实验数据，绘制出在不同问题规模下的，三种方法的时间随问题规模的变化情况，如图3.1所
示，并进行进一步分析
根据测试数据我们可以看出，在 N<300 的规模下，逐行或逐列的访问方式在效率上差别不大，但
采用循环展开的优化算法能够取得较大的性能提升。而当 300<N<3000 时，逐行访问要比逐列访问的
增长速度慢，但在此规模下，三种方法的增长趋势还是基本相同的。当 N>3000 之后，逐列访问的增
长速度要明显超越了逐行访问的方法，证明此时 cache 优化起到了显著作用。
由数据可以进行理论分析，由于 CPU 的 L1-L3 的各级 cache 大小分别为 64KB，512KB 和 48MB，
对于数组元素为 unsigned long long int 而言，每一个元素占据 10 个字节，因此，填满各级 cache 的
元素规模大概在 80，200，2000。而图像的大概走势也恰恰能够印证这一假设，即在 N<300 下，尽管
普通方法的 L1 cache 命中率可能已经很低，但是由于 L2 cache 的访问速度相对比较快，所以访存速
度对整体的时间影响不太大，即两种逐行和逐列访问的方法效率差别不大。而当 300<N<3000 这个区
间上时，L2 cache 的命中率也会不断下降，使得逐列访问的方法被迫大更大的 L3 cache 中去寻找数
据，这就导致了较大的访存开销，也可以看出两种方法的差距逐渐显现出来。而当数据规模超过 3000
之后，逐列访问的 L3 cache 命中率也会降到很低，也就是说逐列访问方法被迫到内存中去寻找数据，
这导致的访存开销是非常大的，因此也使得两种方法的访问效率产生了显著的差异。
为了证明上述假设，我们分别采集了数据规模在 80，300 和 4000 的两组实验，通过 VTune 分析其各级缓存的访存次数和命中率，如表2所示。当 N=80 时，可以看到，逐列访问方法的 L1 cache 未
命中率要显著高于逐行访问，导致其 L2 cache 的访问次数增多，但 L2 cache 大部分情况命中，所以
两种方法效率差距不大。当 N=300 时，逐列访问 L2 cache 未命中率要显著高于逐行访问，导致其 L3
cache 的访问次数也显著高于逐行访问，但 L3 cache 绝大情况下都已命中。而 L3 cache 的访问时间相
对开销较大，因此两种方法的时间差异开始显现出来。当 N=4000 时，我们可以看到，逐列访问方法
的的 L3 cache 访问次数要远远高出逐行访问的方法，而且 L3 cache 的未命中率也高达 27.1%，需要
由大量的内存访问，时间开销会非常大，这也是两种方法产生巨大差距的主要原因。
对于循环展开方法对逐列访问方式的优化，由于循环展开可以在一个循环周期内利用多条流水线
并行执行相同指令，因此能够在一定程度上优化代码运行效率。这一点通过比较两种方法的 CPI 也能
够得到印证，如表3所示。

2
对于给定的问题，要求计算 N 个数的和，对于常规的顺序算法而言，由于每次都是在同一个累加
变量上进行累加，导致只能调用 CPU 的一条流水线进行处理，无法充分发挥 CPU 超标量优化的性
能，因此考虑使用多链路的方法对传统的链式累加方法进行改进，即设置多个临时变量，在一个循环
内同时用着多个临时变量对多个不同的位置进行累加，达到多个位置并行累加的效果，同时还能够减
少循环遍历的步长，降低循环开销。由于多链路方法使用了循环展开技术在一定程度上降低了循环的
额外开销，为了保证实验的准确性，我们对普通的链式累加方法也要进行同样比例的循环展开，控制
实验的可变因素，使得实验结果具有合理的对比性。
通过对比在不同实验规模下的两种方法的运行时间，探究优化加速比同问题规模的变化情况，并
分析其中的内在原因。此外，还将会探究在 x86 架构下，Windows 和 Linux 两种系统对于处理同样规
模的问题所需要消耗的绝对时间，以及优化的加速比的情况。
为了方便算法的实现，我们的所有问题规模都取成 2 的 n 次幂，由于当问题规模较小的时候，两
种算法并没有显著的时间效率差异，因此我们直接扩大了问题规模，分别测试了从 n=9 到 28 之间的
20 组数据，如表4所示。由表中的数据可以看出，无论是链式累加的方法还是多链路展开的方法，由于都属于线性时间效
率的方法，因此随着问题规模的翻倍，时间也近似翻倍。采用双链路展开的超标量优化方法的时间效
率要显著高于普通的链式累加方法，这是因为，通过多链路的方法，将相互联系的累加解耦成了两路
不相关的问题，使得 CPU 能够同时调用两条流水线处理问题，实现超标量优化的目的。为了能够证明
超标量优化确实起到了作用，我们检测了链式累加方法和多链路累加方法的 CPI，如表5所示。根据表
中的数据，可以得到，多链路累加的方法所达到的 CPI 要低于链式累加，也就是说在同一个时钟周期
内，多链路累加方法执行的指令数要多于链式累加，这也证明了我们确实调用了 CPU 的多条流水线
实现了超标量优化的目的。
为了进一步探究超标量优化的加速比，我们计算了优化加速比同问题规模的变化情况，如图3.2所
示。
通过实验数据可以发现，当问题规模较小的时候，算法的优化加速比比较低，但随着问题规模的
增大，算法的优化加速比呈现先增加再保持最后降低的趋势。其增加的原因主要是由于多链路方法将
累加拆分成了两个不相关的部分，能够利用 CPU 的超标量优化，使得算法获得逐渐增高的加速比。当
问题问题规模 2
10 到 2
17 之间的时候，优化加速比基本保持一个稳定的状态，说明此时的超标量优化
已经达到一个上限。而当问题规模超过 2
17 后，整体呈现一个下降的趋势，猜测是由于问题规模过大，
导致缓存不足，由于需要经常进行内外存的访问，导致了较大的访存开销，而这个访存开销占据了程序
运行的大部分时间，所以超标量优化的效果被一定程度上减弱。为了验证上述想法，我们利用 VTune
对下降最显著的点分析各级缓存的访问和命中情况，选取了 2
22 这个点分析，并与 2
12 这个最高点进
行对比，如表6所示。
通过数据可以得到，当问题规模在 2
12 时，所有的问题几乎全部在 L1 cache 命中，L2 cache 和 L3
cache 在过程中几乎没有访问，而且命中率也几乎在 100%。而当问题规模达到 2
22 时，虽然 L1 cache
的命中率还是接近 100%，但是已经有了很多的 L2 cache 和 L3 cache 访问，而且 L3 cache 的命中率
只有 80%，会出现很多的内存访问，这将会极大的影响程序运行的时间，因此印证了我们上面的猜测。
总结


